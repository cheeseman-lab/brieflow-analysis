{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Aggregate Module Params\n",
    "\n",
    "This notebook should be used as a test for ensuring correct aggregate parameters before aggregate processing.\n",
    "Cells marked with <font color='red'>SET PARAMETERS</font> contain crucial variables that need to be set according to your specific experimental setup and data organization.\n",
    "Please review and modify these variables as needed before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Fixed parameters for aggregate module\n",
    "\n",
    "- `CONFIG_FILE_PATH`: Path to a Brieflow config file used during processing. Absolute or relative to where workflows are run from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nfrom itertools import product\nimport random\n\nimport yaml\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom lib.shared.file_utils import get_filename, load_parquet_subset\nfrom lib.aggregate.cell_data_utils import split_cell_data, channel_combo_subset\nfrom lib.aggregate.cell_classification import CellClassifier\nfrom lib.aggregate.montage_utils import create_cell_montage, add_filenames\nfrom lib.aggregate.filter import (\n    query_filter,\n    perturbation_filter,\n    missing_values_filter,\n    intensity_filter,\n)\nfrom lib.aggregate.align import (\n    prepare_alignment_data,\n    pca_variance_plot,\n    embed_by_pca,\n    tvn_on_controls,\n)\nfrom lib.aggregate.aggregate import aggregate\nfrom lib.aggregate.eval_aggregate import (\n    nas_summary,\n    summarize_cell_data,\n    plot_feature_distributions,\n)\nfrom lib.shared.configuration_utils import CONFIG_FILE_HEADER, convert_tuples_to_lists\n\nrandom.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Testing on subset of data\n",
    "\n",
    "- `TEST_PLATE`: Plate used for testing configuration \n",
    "- `TEST_WELL_1`: First well identifier used for testing configuration\n",
    "- `TEST_WELL_2`: Second well identifier used for testing configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLATE = None\n",
    "TEST_WELL_1 = None\n",
    "TEST_WELL_2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file and determine root path\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "ROOT_FP = Path(config[\"all\"][\"root_fp\"])\n",
    "\n",
    "# Load subset of data\n",
    "# Takes ~1 minute\n",
    "merge_final_fp = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL_1}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data = load_parquet_subset(merge_final_fp, n_rows=25000)\n",
    "\n",
    "merge_final_fp_2 = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL_2}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data_2 = load_parquet_subset(merge_final_fp_2, n_rows=25000)\n",
    "\n",
    "cell_data = pd.concat([cell_data, cell_data_2], ignore_index=True)\n",
    "cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cell_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## <font color='red'>SET PARAMETERS</font>\n\n### Cell Data Metadata\n\n- `METADATA_COLS_FP`: Path to TSV to store metadata cols.\n- `METADATA_COLS`: Loaded from config (set in notebook 7). Modify if needed."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load classification settings from config (set in notebook 7)\nfrom lib.phenotype.constants import DEFAULT_METADATA_COLS\n\nif \"classify\" in config:\n    METADATA_COLS_FP = config[\"classify\"][\"metadata_cols_fp\"]\n    CLASSIFIER_PATH = config[\"classify\"][\"classifier_path\"]\n    CONFIDENCE_THRESHOLDS = config[\"classify\"][\"confidence_thresholds\"]\n    CLASS_TITLE = config[\"classify\"][\"class_title\"]  # e.g., \"cell_stage\"\n    CLASS_MAPPING = config[\"classify\"][\"class_mapping\"]  # e.g., {\"label_to_class\": {1: \"Mitotic\", 2: \"Interphase\"}}\n    # Load metadata columns from file\n    METADATA_COLS = pd.read_csv(METADATA_COLS_FP, header=None, sep=\"\\t\")[0].tolist()\n    print(f\"Loaded metadata columns from classify config: {METADATA_COLS_FP}\")\nelse:\n    # Fall back to defaults if classify notebook was skipped\n    METADATA_COLS_FP = \"config/cell_data_metadata_cols.tsv\"\n    CLASSIFIER_PATH = None\n    CONFIDENCE_THRESHOLDS = None\n    CLASS_TITLE = None\n    CLASS_MAPPING = None\n    METADATA_COLS = DEFAULT_METADATA_COLS.copy()\n    # Save default metadata cols to file for pipeline\n    pd.Series(METADATA_COLS).to_csv(METADATA_COLS_FP, index=False, header=False, sep=\"\\t\")\n    print(f\"Using DEFAULT_METADATA_COLS (classify notebook was skipped)\")\n    print(f\"Saved to: {METADATA_COLS_FP}\")\n\n# Always show the metadata columns so user can verify\nprint(f\"\\nClassifier path: {CLASSIFIER_PATH}\")\nprint(f\"Class title: {CLASS_TITLE}\")\nprint(f\"Class mapping: {CLASS_MAPPING}\")\nprint(f\"Confidence thresholds: {CONFIDENCE_THRESHOLDS}\")\nprint(f\"\\n{len(METADATA_COLS)} metadata columns:\")\nfor col in METADATA_COLS:\n    print(f\"  - {col}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split cell data into metadata and features\nmetadata, features = split_cell_data(cell_data, METADATA_COLS)\nprint(metadata.shape, features.shape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Split cells into classes\n",
    "\n",
    "- `CLASSIFIER_PATH`: Path to pickled Python object that can take a cell data dataframe and output cell classes\n",
    "\n",
    "### Evaluate splitting\n",
    "\n",
    "- `COLLAPSE_COLS`: Cell data columns to collapse on when creating a summary of cell counts. This will show the number of cells in each cell class for these particular columns. Ex: `[\"cell_barcode_0\", \"gene_symbol_0\"]`.\n",
    "- `MONTAGE_CHANNEL`: Channel to use for montage generation. Usually `DAPI`.\n",
    "\n",
    "**Notes**: \n",
    "- We generate cell classes for each of the classes listed in the classifier and an \"all\" class. So for a classifier that splits by mitotic or interphase the final classes will be `[\"mitotic\", \"interphase\", \"all\"]`.\n",
    "- You must import necessary packages for the classifier in this notebook and add them to `scripts/aggregate/split_datasets.py` as well. Ex `import numpy as np` if the classifier requires `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Classification settings loaded from config above\n# Override here if needed:\n# CLASSIFIER_PATH = None\n# CONFIDENCE_THRESHOLD = None\n\nMONTAGE_CHANNEL = None\nCOLLAPSE_COLS = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "classifier = CellClassifier.load(CLASSIFIER_PATH)\n\n# Classify cells with safety check\nif classifier is not None:\n    print(\"Applying cell classification...\")\n    classified_metadata, classified_features = classifier.classify_cells(metadata, features)\n    \n    # Add standard 'class' and 'confidence' columns using the mapping\n    confidence_col = f\"{CLASS_TITLE}_confidence\"\n    if CLASS_MAPPING and \"label_to_class\" in CLASS_MAPPING:\n        label_to_class = CLASS_MAPPING[\"label_to_class\"]\n        # Convert numeric IDs to string labels\n        classified_metadata[\"class\"] = classified_metadata[CLASS_TITLE].map(\n            lambda x: label_to_class.get(x, label_to_class.get(str(x), str(x)))\n        )\n    else:\n        # No mapping available, use numeric values as strings\n        classified_metadata[\"class\"] = classified_metadata[CLASS_TITLE].astype(str)\n    \n    # Add standard confidence column\n    classified_metadata[\"confidence\"] = classified_metadata[confidence_col]\nelse:\n    print(\"No classifier specified - using all cells as single class\")\n    classified_metadata, classified_features = metadata, features\n\n# Create config var for cell classes\nif \"class\" in classified_metadata.columns:\n    CELL_CLASSES = list(classified_metadata[\"class\"].unique())\nelse:\n    CELL_CLASSES = []\n\n# Show cell class counts and distribution\nif CELL_CLASSES:\n    print(\"\\nCell class counts:\")\n    print(classified_metadata[\"class\"].value_counts())\n\n    print(\"\\nCell class confidences:\")\n    classified_metadata[\"confidence\"].hist()\nelse:\n    print(\"No cell classes available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "cell_classes = list(classified_metadata[\"class\"].unique()) + [\"all\"]\n\nclassified_metadata_copy = classified_metadata.copy(deep=True)\nclassified_metadata_copy = add_filenames(classified_metadata_copy, ROOT_FP)\n\n# Create a dictionary of DataFrames for each cell class\ncell_class_dfs = {\n    cell_class: classified_metadata_copy[\n        classified_metadata_copy[\"class\"] == cell_class\n    ]\n    for cell_class in CELL_CLASSES\n}\n\n# Define sorting directions and titles\ntitle_templates = {\n    True: \"Lowest Confidence {cell_class} Cells - {channel}\",\n    False: \"Highest Confidence {cell_class} Cells - {channel}\",\n}\n\n# Generate montages dynamically\nmontages, titles = [], []\nfor cell_class, cell_df in cell_class_dfs.items():\n    for ascending in [True, False]:\n        montage = create_cell_montage(\n            cell_data=cell_df,\n            channels=config[\"phenotype\"][\"channel_names\"],\n            selection_params={\n                \"method\": \"sorted\",\n                \"sort_by\": \"confidence\",\n                \"ascending\": ascending,\n            },\n        )[MONTAGE_CHANNEL]\n        montages.append(montage)\n        titles.append(\n            title_templates[ascending].format(\n                cell_class=cell_class, channel=MONTAGE_CHANNEL\n            )\n        )\n\n# Determine figure size dynamically\nnum_rows = len(CELL_CLASSES)\nfig, axes = plt.subplots(num_rows, 2, figsize=(10, 3 * num_rows))\n\n# Display montages\nfor ax, title, montage in zip(axes.flat, titles, montages):\n    ax.imshow(montage, cmap=\"gray\")\n    ax.set_title(title, fontsize=14)\n    ax.axis(\"off\")\n\nprint(\"Montages of cell classes:\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Split cell data summary:\")\nsummary_df = summarize_cell_data(classified_metadata, CELL_CLASSES, COLLAPSE_COLS)\ndisplay(summary_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Aggregate by channel combos\n",
    "\n",
    "- `CHANNEL_COMBOS`: Combinations of channels to aggregate by. This is a list of lists with channel names, ex `[[\"DAPI\", \"CENPA\"], [\"DAPI\", \"WGA\"]]`.\n",
    "- `AGGREGATE_COMBO_FP`: Location of aggregate combinations dataframe.\n",
    "- `TEST_CELL_CLASS`: Cell class to configure aggregate params with. Can be any of the cell classes or `all`.\n",
    "- `TEST_CHANNEL_COMBO`: Channel combo to configure aggregate params with; must be one of the channel combos. Ex `[\"DAPI\", \"CENPA\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_COMBOS = None\n",
    "AGGREGATE_COMBO_FP = \"config/aggregate_combo.tsv\"\n",
    "\n",
    "TEST_CELL_CLASS = None\n",
    "TEST_CHANNEL_COMBO = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine cell classes and channel combos\n",
    "channel_combos = [\"_\".join(combo) for combo in CHANNEL_COMBOS]\n",
    "\n",
    "# Load merge wildcard combos\n",
    "MERGE_COMBO_FP = Path(config[\"merge\"][\"merge_combo_fp\"])\n",
    "merge_wildcard_combos = pd.read_csv(MERGE_COMBO_FP, sep=\"\\t\")\n",
    "\n",
    "# Generate aggregate wildcard combos\n",
    "aggregate_wildcard_combos = pd.DataFrame(\n",
    "    product(\n",
    "        merge_wildcard_combos.itertuples(index=False, name=None),\n",
    "        cell_classes,\n",
    "        channel_combos,\n",
    "    ),\n",
    "    columns=[\"plate_well\", \"cell_class\", \"channel_combo\"],\n",
    ")\n",
    "aggregate_wildcard_combos[[\"plate\", \"well\"]] = pd.DataFrame(\n",
    "    aggregate_wildcard_combos[\"plate_well\"].tolist(),\n",
    "    index=aggregate_wildcard_combos.index,\n",
    ")\n",
    "aggregate_wildcard_combos = aggregate_wildcard_combos.drop(columns=\"plate_well\")\n",
    "\n",
    "# Save aggregate wildcard combos\n",
    "aggregate_wildcard_combos.to_csv(AGGREGATE_COMBO_FP, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Aggregate wildcard combos:\")\n",
    "aggregate_wildcard_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset cell class\n",
    "if TEST_CELL_CLASS != \"all\":\n",
    "    cell_class_mask = classified_metadata[\"class\"] == TEST_CELL_CLASS\n",
    "    class_metadata = classified_metadata[cell_class_mask]\n",
    "    class_features = classified_features[cell_class_mask]\n",
    "else:\n",
    "    class_metadata = classified_metadata\n",
    "    class_features = classified_features\n",
    "\n",
    "# subset features\n",
    "all_channels = config[\"phenotype\"][\"channel_names\"]\n",
    "class_features = channel_combo_subset(class_features, TEST_CHANNEL_COMBO, all_channels)\n",
    "\n",
    "# copy metadata and features for later eval\n",
    "dataset_metadata = class_metadata.copy()\n",
    "dataset_features = class_features.copy()\n",
    "\n",
    "# preview metadata and features\n",
    "display(class_metadata)\n",
    "display(class_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Perturbation filtering\n",
    "\n",
    "- `FILTER_QUERIES`: Queries to use for custom filtering; ex: `[\"mapped_single_gene == True\", \"cell_quality_score > 0.8\"]`. Can be left as `None` for no filtering.\n",
    "- `PERTURBATION_NAME_COL`: Name of column used to identify perturbations. This is the column that aggregation takes place on. Ex \"gene_symbol_0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_QUERIES = None\n",
    "PERTURBATION_NAME_COL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_metadata, qf_features = query_filter(class_metadata, class_features, FILTER_QUERIES)\n",
    "\n",
    "pf_metadata, pf_features = perturbation_filter(\n",
    "    qf_metadata, qf_features, PERTURBATION_NAME_COL\n",
    ")\n",
    "print(f\"Unique populations: {metadata[PERTURBATION_NAME_COL].nunique()}\")\n",
    "\n",
    "summary_df, fig = nas_summary(pf_features)\n",
    "print(summary_df[summary_df[\"percent_na\"] > 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Missing value filtering\n",
    "\n",
    "- `DROP_COLS_THRESHOLD`: Threshold of NA values above which an entire column is dropped. Usually `0.1`\n",
    "- `DROP_ROWS_THRESHOLD`: Threshold of NA values above which an entire row is dropped. Usually `0.01`\n",
    "- `IMPUTE`: Whether or not to impute remaining missing values. Usually `True`\n",
    "\n",
    "**Note**: All NAs must be dropped or imputed to perform feature alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS_THRESHOLD = None\n",
    "DROP_ROWS_THRESHOLD = None\n",
    "IMPUTE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by missing values\n",
    "mvf_metadata, mvf_features = missing_values_filter(\n",
    "    pf_metadata,\n",
    "    pf_features,\n",
    "    drop_cols_threshold=DROP_COLS_THRESHOLD,\n",
    "    drop_rows_threshold=DROP_ROWS_THRESHOLD,\n",
    "    impute=True,\n",
    ")\n",
    "\n",
    "mvf_metadata.shape, mvf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Intensity filtering\n",
    "\n",
    "- `CONTAMINATION`: Expected proportion of outliers in dataset. Usually `0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAMINATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by intensity outliers\n",
    "if_metadata, if_features = intensity_filter(\n",
    "    mvf_metadata,\n",
    "    mvf_features,\n",
    "    config[\"phenotype\"][\"channel_names\"],\n",
    "    CONTAMINATION,\n",
    ")\n",
    "\n",
    "if_metadata.shape, if_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Perturbation Score Filtering\n",
    "\n",
    "One can optionally assigned perturbation scores to each cell with perturbation scoring and filtering. During scoring, we train a Logistic Regression classifier on perturbed and control cell features. The result of this training and classification process is:\n",
    "- `AUC Score` assinged per perturbation: this represents the classifier's ability to distinguish between perturbed and control cells -> higher AUC score, more clear effect on perturbation level\n",
    "- `Perturbation Probability` assigned per cell: this represents the classifier's assigned probability of the cell being perturbed -> higher perturbation probability, more clear effect on cell level\n",
    "\n",
    "Once perturbation scores are assigned, one can optionally filter with a probability or percentile threshold before aggregation. Control cells will not be filtered.\n",
    "\n",
    "Configure perturbation score filtering with:\n",
    "- `SKIP_PERTURBATION_SCORE`: Whether or not to skip perturbation scoring entirely. Usually `False`\n",
    "- `PS_PROBABILITY_THRESHOLD`: Probability threshold above which to keep perturbed cells. Usually `0.75`\n",
    "- `PS_PERCENTILE_THRESHOLD`: Percentile threshold above which to keep perturbed cells. Usually `0.75`\n",
    "\n",
    "Leave `PS_PROBABILITY_THRESHOLD` and `PS_PERCENTILE_THRESHOLD` as `None` if no filtering is desired.\n",
    "\n",
    "**Notes:** \n",
    "- We don't test perturbation scoring within this notebook as it requires sampling many cells with the same perturbation across plates and wells\n",
    "- Both types of threshold will remove cells, and probability threshold can potentially eliminate all cells in a perturbation\n",
    "- Perturbation scoring can take hours per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_PERTURBATION_SCORE = False\n",
    "PS_PROBABILITY_THRESHOLD = None\n",
    "PS_PERCENTILE_THRESHOLD = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Prepare alignment data\n",
    "\n",
    "- `BATCH_COLS`: Which columns of metadata have batch-specific information. Usually `[\"plate\", \"well\"]`.\n",
    "- `CONTROL_KEY`: Name of perturbation in `PERTURBATION_NAME_COL` that indicates a control cell.\n",
    "- `PERTURBATION_ID_COL`: Name of column that identifies unique perturbations. Only needed if you want your controls to have different perturbation names, ex `cell_barcode_0`. Otherwise, can leave this as `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_COLS = None\n",
    "CONTROL_KEY = None\n",
    "PERTURBATION_ID_COL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_metadata, pad_features = prepare_alignment_data(\n",
    "    if_metadata,\n",
    "    if_features,\n",
    "    BATCH_COLS,\n",
    "    PERTURBATION_NAME_COL,\n",
    "    CONTROL_KEY,\n",
    "    PERTURBATION_ID_COL,\n",
    ")\n",
    "\n",
    "n_components, fig = pca_variance_plot(pad_features, variance_threshold=0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Align and aggregate\n",
    "\n",
    "- `VARIANCE_OR_NCOMP`: Variance or number of components to keep after PCA.\n",
    "- `NUM_ALIGN_BATCHES`: Number of batches to use when aligning, usually `1`. Increase this if you are running out of memory while aligning. We were able to barely fit 8 plates with 6 wells each in 1 TB of memory with `NUM_ALIGN_BATCHES=1`.\n",
    "- `AGG_METHOD`: Method used to aggregate features. Can be `mean` or `median`. Usually `median`.\n",
    "\n",
    "While we use a simplified aggregate method in the notebook, the way this works during a normal run is:\n",
    "1) Take a subset of 1,000,000 cells, or the entire dataset, whichever is smaller and compute a PCA transform with `VARIANCE_OR_NCOMP`.\n",
    "2) Subset the entire dataset `NUM_BATCHES` number of times and align cells in this batch.\n",
    "3) Aggregate across all aligned cell data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_OR_NCOMP = None\n",
    "NUM_ALIGN_BATCHES = None\n",
    "AGG_METHOD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = embed_by_pca(\n",
    "    pad_features,\n",
    "    pad_metadata,\n",
    "    variance_or_ncomp=VARIANCE_OR_NCOMP,\n",
    "    batch_col=\"batch_values\",\n",
    ")\n",
    "\n",
    "tvn_normalized = tvn_on_controls(\n",
    "    pca_embeddings, pad_metadata, PERTURBATION_NAME_COL, CONTROL_KEY, \"batch_values\"\n",
    ")\n",
    "\n",
    "aggregated_embeddings, aggregated_metadata = aggregate(\n",
    "    tvn_normalized, pad_metadata, PERTURBATION_NAME_COL, AGG_METHOD\n",
    ")\n",
    "\n",
    "feature_columns = [f\"PC_{i}\" for i in range(tvn_normalized.shape[1])]\n",
    "\n",
    "tvn_normalized_df = pd.DataFrame(\n",
    "    tvn_normalized, index=pad_metadata.index, columns=feature_columns\n",
    ")\n",
    "aligned_cell_data = pd.concat([pad_metadata, tvn_normalized_df], axis=1)\n",
    "\n",
    "aggregated_embeddings_df = pd.DataFrame(\n",
    "    aggregated_embeddings, index=aggregated_metadata.index, columns=feature_columns\n",
    ")\n",
    "aggregated_cell_data = (\n",
    "    pd.concat([aggregated_metadata, aggregated_embeddings_df], axis=1)\n",
    "    .sort_values(\"cell_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_cols = [\n",
    "    col\n",
    "    for col in dataset_features.columns\n",
    "    if (\"cell_\" in col and col.endswith(\"_mean\"))\n",
    "]\n",
    "pc_cols = [col for col in aggregated_cell_data.columns if col.startswith(\"PC_\")]\n",
    "aligned_feature_cols = random.sample(\n",
    "    pc_cols, k=min(len(original_feature_cols), len(pc_cols))\n",
    ")\n",
    "\n",
    "original_cell_data = pd.concat([dataset_metadata, dataset_features], axis=1)\n",
    "original_cell_data\n",
    "\n",
    "feature_distributions_fig = plot_feature_distributions(\n",
    "    original_feature_cols,\n",
    "    original_cell_data,\n",
    "    aligned_feature_cols,\n",
    "    aligned_cell_data,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "### Generate feature table & bootstrapping\n",
    "\n",
    "Bootstrapping is a statistical resampling method used to determine if perturbations (gene knockdowns, drug treatments, etc.) cause statistically significant changes compared to controls. The method works by:\n",
    "1. Taking your control population and repeatedly resampling it to create thousands of \"null\" distributions\n",
    "2. Comparing each perturbation's effect size against these null distributions\n",
    "3. Calculating p-values based on how often the null distributions produce effects as large as the real perturbation\n",
    "\n",
    "- `FEATURE_NORMALIZATION`: Method for normalizing features before bootstrapping. Options: `\"standard\"` (standard scalar) or `\"mad\"` (median absolute deviation).\n",
    "- `NUM_SIMS`: Number of bootstrap simulations to run for statistical testing. Ex: `100000`.\n",
    "- `EXCLUSION_STRING`: String to exclude certain constructs from analysis (optional). Ex: `\"nontargeting_noncutting_\"` or `None`.\n",
    "- `BOOTSTRAP_CELL_CLASS`: Cell class to run bootstrapping on. Ex: `\"Interphase\"` or `\"all\"`.\n",
    "- `BOOTSTRAP_CHANNEL_COMBO`: Channel combination to run bootstrapping on. Ex: `\"DAPI_Ki-67_COXIV_Caspase-3_WGA_aTubulin_Vimentin_gH2AX_Phalloidin\"`.\n",
    "- `PSEUDOGENE_PATTERNS`: Dictionary defining how to group single-construct genes into pseudo-genes for more robust statistical testing. For each pattern category, specify:\n",
    "  - `pattern`: Regex pattern to match gene names\n",
    "  - `constructs_per_pseudogene`: Number of constructs to group together\n",
    "\n",
    "**Note**: Without the combinations configured, bootstrapping will be skipped entirely. Bootstrap analysis can take hours to days depending on `NUM_SIMS` and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NORMALIZATION = \"standard\"\n",
    "NUM_SIMS = None\n",
    "EXCLUSION_STRING = None\n",
    "BOOTSTRAP_CELL_CLASS = None\n",
    "BOOTSTRAP_CHANNEL_COMBO = None\n",
    "PSEUDOGENE_PATTERNS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add aggregate parameters to config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add aggregate section (classifier settings are in config[\"classify\"] from notebook 7)\nconfig[\"aggregate\"] = {\n    \"metadata_cols_fp\": METADATA_COLS_FP,\n    \"collapse_cols\": COLLAPSE_COLS,\n    \"aggregate_combo_fp\": AGGREGATE_COMBO_FP,\n    \"filter_queries\": FILTER_QUERIES,\n    \"perturbation_name_col\": PERTURBATION_NAME_COL,\n    \"drop_cols_threshold\": DROP_COLS_THRESHOLD,\n    \"drop_rows_threshold\": DROP_ROWS_THRESHOLD,\n    \"impute\": IMPUTE,\n    \"contamination\": CONTAMINATION,\n    \"batch_cols\": BATCH_COLS,\n    \"control_key\": CONTROL_KEY,\n    \"perturbation_id_col\": PERTURBATION_ID_COL,\n    \"variance_or_ncomp\": VARIANCE_OR_NCOMP,\n    \"num_align_batches\": NUM_ALIGN_BATCHES,\n    \"agg_method\": AGG_METHOD,\n    \"skip_perturbation_score\": SKIP_PERTURBATION_SCORE,\n    \"ps_probability_threshold\": PS_PROBABILITY_THRESHOLD,\n    \"ps_percentile_threshold\": PS_PERCENTILE_THRESHOLD,\n}\n\nif BOOTSTRAP_CELL_CLASS and BOOTSTRAP_CHANNEL_COMBO:\n    # Normalize to lists for iteration (supports both single values and lists)\n    cell_classes_list = BOOTSTRAP_CELL_CLASS if isinstance(BOOTSTRAP_CELL_CLASS, list) else [BOOTSTRAP_CELL_CLASS]\n    channel_combos_list = BOOTSTRAP_CHANNEL_COMBO if isinstance(BOOTSTRAP_CHANNEL_COMBO, list) else [BOOTSTRAP_CHANNEL_COMBO]\n\n    # Create all combinations\n    BOOTSTRAP_COMBINATIONS = [\n        {\"cell_class\": cc, \"channel_combo\": ch}\n        for cc, ch in product(cell_classes_list, channel_combos_list)\n    ]\n    config[\"aggregate\"].update({\n        \"feature_normalization\": FEATURE_NORMALIZATION,\n        \"num_sims\": NUM_SIMS,\n        \"exclusion_string\": EXCLUSION_STRING,\n        \"bootstrap_combinations\": BOOTSTRAP_COMBINATIONS,\n        \"pseudogene_patterns\": PSEUDOGENE_PATTERNS,\n    })\n\n# Convert tuples to lists before dumping\nsafe_config = convert_tuples_to_lists(config)\n\n# Write the updated configuration\nwith open(CONFIG_FILE_PATH, \"w\") as config_file:\n    # Write the introductory comments\n    config_file.write(CONFIG_FILE_HEADER)\n\n    # Dump the updated YAML structure, keeping markdown comments for sections\n    yaml.dump(safe_config, config_file, default_flow_style=False, sort_keys=False)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brieflow_main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
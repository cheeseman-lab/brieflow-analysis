{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of metadata keys to filename prefixes and data types\n",
    "FILENAME_METADATA_MAPPING = {\n",
    "    \"plate\": [\"P-\", str],\n",
    "    \"well\": [\"W-\", str],\n",
    "    \"tile\": [\"T-\", int],\n",
    "    \"cycle\": [\"C-\", int],\n",
    "    \"gene\": [\"G-\", str],\n",
    "    \"sgrna\": [\"SG-\", str],\n",
    "    \"channel\": [\"CH-\", str],\n",
    "    \"dataset\": [\"DT-\", str],\n",
    "}\n",
    "\n",
    "\n",
    "def get_filename(data_location: dict, info_type: str, file_type: str) -> str:\n",
    "    \"\"\"Generate a structured filename based on data location, information type, and file type.\n",
    "\n",
    "    Args:\n",
    "        data_location (dict): Dictionary containing location info like well, tile, and cycle.\n",
    "        info_type (str): Type of information (e.g., 'cell_features', 'sbs_reads').\n",
    "        file_type (str): File extension/type (e.g., 'tsv', 'parquet', 'tiff').\n",
    "\n",
    "    Returns:\n",
    "        str: Structured filename.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "\n",
    "    for metadata_key, metadata_value in data_location.items():\n",
    "        if metadata_key in FILENAME_METADATA_MAPPING:\n",
    "            prefix, _ = FILENAME_METADATA_MAPPING[metadata_key]\n",
    "            parts.append(f\"{prefix}{metadata_value}\")\n",
    "        else:\n",
    "            print(f\"Unknown metadata key: {metadata_key}\")\n",
    "\n",
    "    prefix = \"_\".join(parts)\n",
    "    filename = (\n",
    "        f\"{prefix}__{info_type}.{file_type}\" if prefix else f\"{info_type}.{file_type}\"\n",
    "    )\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def load_parquet_subset(full_df_fp, n_rows=50000):\n",
    "    \"\"\"Load a fixed number of rows from an parquet file without loading entire file into memory.\n",
    "\n",
    "    Args:\n",
    "        full_df_fp (str): Path to parquet file.\n",
    "        n_rows (int): Number of rows to get.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Subset of the data with combined blocks.\n",
    "    \"\"\"\n",
    "    print(f\"Reading first {n_rows:,} rows from {full_df_fp}\")\n",
    "\n",
    "    # read the first n_rows of the file path\n",
    "    df = ParquetFile(full_df_fp)\n",
    "    row_subset = next(df.iter_batches(batch_size=n_rows))\n",
    "    df = pa.Table.from_batches([row_subset]).to_pandas()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLATE = 1\n",
    "TEST_WELL = \"A1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file and determine root path\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "ROOT_FP = Path(config[\"all\"][\"root_fp\"])\n",
    "\n",
    "# Load subset of data\n",
    "# Takes ~1 minute\n",
    "merge_final_fp = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data = load_parquet_subset(merge_final_fp)\n",
    "\n",
    "display(cell_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 20 columns; use to set parameters below.\")\n",
    "for index, col in enumerate(cell_data.columns[:20]):\n",
    "    print(index, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation_filter(\n",
    "    cell_data,\n",
    "    perturbation_name_col,\n",
    "    perturbation_multi_col=None,\n",
    "    filter_single_pert=False,\n",
    "):\n",
    "    \"\"\"Clean cell data by removing cells without perturbation assignments and optionally filtering for single-gene cells.\n",
    "\n",
    "    Args:\n",
    "        cell_data (pd.DataFrame): Raw dataframe containing cell measurements.\n",
    "        perturbation_name_col (str): Column name containing perturbation assignments.\n",
    "        perturbation_multi_col (str): If not None, only keep cells with perturbation_multi_col=True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataframe.\n",
    "    \"\"\"\n",
    "    # Remove cells without perturbation assignments\n",
    "    clean_cell_data = cell_data[cell_data[perturbation_name_col].notna()].copy()\n",
    "    print(f\"Found {len(clean_cell_data)} cells with assigned perturbations\")\n",
    "\n",
    "    if filter_single_pert:\n",
    "        # Filter for single-gene cells if requested\n",
    "        clean_cell_data = clean_cell_data[\n",
    "            clean_cell_data[perturbation_multi_col] == True\n",
    "        ]\n",
    "        print(f\"Kept {len(clean_cell_data)} cells with single gene assignments\")\n",
    "    else:\n",
    "        # Warn about multi-gene cells if not filtering\n",
    "        multi_pert_cells = len(\n",
    "            clean_cell_data[clean_cell_data[perturbation_multi_col] == False]\n",
    "        )\n",
    "        if multi_pert_cells > 0:\n",
    "            print(\n",
    "                f\"WARNING: {multi_pert_cells} cells have multiple perturbation assignments\"\n",
    "            )\n",
    "\n",
    "    return clean_cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERTURBATION_NAME_COL = \"gene_symbol_0\"\n",
    "PERTURBATION_MULTI_COL = \"mapped_single_gene\"\n",
    "FILTER_SINGLE_PERT = False\n",
    "\n",
    "perturbation_filtered = perturbation_filter(\n",
    "    cell_data, PERTURBATION_NAME_COL, PERTURBATION_MULTI_COL, FILTER_SINGLE_PERT\n",
    ")\n",
    "print(f\"Unique populations: {perturbation_filtered[PERTURBATION_NAME_COL].nunique()}\")\n",
    "perturbation_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_START_IDX = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_na_matrix(df):\n",
    "    \"\"\"\n",
    "    Creates a visualization matrix showing which columns have NA values and in which rows.\n",
    "    No color bar is displayed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to analyze for NA values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure or None\n",
    "        The figure object if NAs are found, None otherwise\n",
    "    \"\"\"\n",
    "    # Get only columns with at least one NA\n",
    "    cols_with_na = df.columns[df.isna().any()].tolist()\n",
    "    \n",
    "    if not cols_with_na:\n",
    "        print(\"No columns with NA values found in the DataFrame.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a smaller DataFrame with only columns containing NAs\n",
    "    na_df = df[cols_with_na].isna()\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title(f\"NA Values Matrix ({len(cols_with_na)} columns with missing values)\")\n",
    "    \n",
    "    # Create heatmap - True (NA) values will be colored, without color bar\n",
    "    ax = sns.heatmap(na_df, cmap='viridis', cbar=False)\n",
    "    \n",
    "    # Display column names on x-axis, rotated for readability\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add summary information\n",
    "    na_counts = df[cols_with_na].isna().sum()\n",
    "    na_percent = (na_counts / len(df)) * 100\n",
    "    \n",
    "    print(f\"Columns with high NA value percent:\")\n",
    "    for col, count, pct in zip(cols_with_na, na_counts, na_percent):\n",
    "        if pct > 10:\n",
    "            print(f\"  - {col}: {count} NAs ({pct:.2f}%)\")\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "fig = visualize_na_matrix(perturbation_filtered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "\n",
    "def intensity_filter(\n",
    "    cell_data, feature_start_idx, channel_names=None, contamination=0.01\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses EllipticEnvelope to filter outliers by channel intensities.\n",
    "\n",
    "    Derived from Recursion's EFAAR pipeline: https://github.com/recursionpharma/EFAAR_benchmarking/blob/60df3eb267de3ba13b95f720b2a68c85f6b63d14/efaar_benchmarking/efaar.py#L295\n",
    "\n",
    "    Args:\n",
    "        cell_data (pd.DataFrame): Cell data dataframe.\n",
    "        feature_start_idx (int): Index of the first feature column.\n",
    "        channel_names (list[str], optional): A list of channel names to use for intensity filtering. Defaults to None.\n",
    "        contamination (float, optional): The proportion of outliers to expect. Defaults to 0.01.\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered cell data dataframe.\n",
    "    \"\"\"\n",
    "    # Identify feature cols\n",
    "    feature_cols = perturbation_filtered.columns[feature_start_idx:].tolist()\n",
    "\n",
    "    # Determine intensity columns\n",
    "    intensity_cols = [\n",
    "        col\n",
    "        for col in feature_cols\n",
    "        if any(col.endswith(f\"_{channel}_mean\") for channel in channel_names)\n",
    "    ]\n",
    "\n",
    "    # Fit EllipticEnvelope to intensity cols and get mask\n",
    "    mask = EllipticEnvelope(contamination=contamination, random_state=42).fit_predict(\n",
    "        cell_data[intensity_cols]\n",
    "    )\n",
    "\n",
    "    # Return filtered cell data\n",
    "    return cell_data[mask == 1].reset_index(drop=True)\n",
    "\n",
    "# Load channel names\n",
    "channel_names = config[\"phenotype\"][\"channel_names\"]\n",
    "\n",
    "intensity_filtered = intensity_filter(\n",
    "    perturbation_filtered, FEATURE_START_IDX, channel_names\n",
    ")\n",
    "intensity_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def missing_values_filter(cell_data, feature_start_idx, impute=True, drop_rows=False, drop_cols=False, drop_cols_threshold=None):\n",
    "    \"\"\"Filter cell data by handling missing values through dropping or imputation.\n",
    "\n",
    "    Args:\n",
    "        cell_data (pd.DataFrame): Raw dataframe containing cell measurements.\n",
    "        feature_start_idx (int): Index of the first feature column.\n",
    "        impute (bool): Whether to impute remaining missing values after dropping. Defaults to True.\n",
    "        drop_rows (bool): Whether to drop all rows with any missing values. Defaults to False.\n",
    "        drop_cols (bool): Whether to drop all columns with any missing values. Defaults to False.\n",
    "        drop_cols_threshold (float, optional): If provided, drops columns with NaN proportion >= threshold.\n",
    "                                              This overrides drop_cols if both are specified.\n",
    "                                              Range: 0.0-1.0. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered dataframe with handled missing values.\n",
    "    \"\"\"\n",
    "    # Get features\n",
    "    metadata = cell_data.iloc[:, :feature_start_idx].copy()\n",
    "    features = cell_data.iloc[:, feature_start_idx:].copy()\n",
    "    \n",
    "    # Get columns with missing values\n",
    "    cols_with_na = features.columns[features.isna().any()].tolist()\n",
    "    \n",
    "    if not cols_with_na:\n",
    "        return cell_data\n",
    "    \n",
    "    # Perform dropping operations if requested\n",
    "    if drop_rows:\n",
    "        # Drop rows with any missing values\n",
    "        original_row_count = features.shape[0]\n",
    "        features.dropna(axis=0, inplace=True)\n",
    "        print(f\"Dropped {original_row_count - features.shape[0]} rows with missing values\")\n",
    "        \n",
    "        # Update metadata to match remaining rows\n",
    "        metadata = metadata.loc[features.index]\n",
    "    \n",
    "    # Handle column dropping based on parameters\n",
    "    if drop_cols_threshold is not None:\n",
    "        # Calculate proportion of NaN values in each column\n",
    "        na_proportions = features.isna().mean()\n",
    "        \n",
    "        # Identify columns to drop based on threshold\n",
    "        cols_to_drop = na_proportions[na_proportions >= drop_cols_threshold].index.tolist()\n",
    "        \n",
    "        if cols_to_drop:\n",
    "            print(f\"Dropping {len(cols_to_drop)} columns with ≥{drop_cols_threshold*100}% missing values\")\n",
    "            features.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    if drop_cols:\n",
    "        # Drop all columns with any missing values\n",
    "        print(f\"Dropping all {len(cols_with_na)} columns with any missing values\")\n",
    "        features.drop(columns=cols_with_na, inplace=True)\n",
    "    \n",
    "    # Impute remaining missing values if requested\n",
    "    if impute:\n",
    "        # Get updated list of columns with missing values\n",
    "        remaining_cols_with_na = features.columns[features.isna().any()].tolist()\n",
    "        \n",
    "        if remaining_cols_with_na:\n",
    "            print(f\"Imputing {len(remaining_cols_with_na)} columns with remaining missing values\")\n",
    "            \n",
    "            # Store index for later reconstruction\n",
    "            index = features.index\n",
    "            \n",
    "            # Apply imputation only to columns with missing values\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            features[remaining_cols_with_na] = pd.DataFrame(\n",
    "                imputer.fit_transform(features[remaining_cols_with_na]),\n",
    "                columns=remaining_cols_with_na,\n",
    "                index=index\n",
    "            )\n",
    "    \n",
    "    # Combine metadata and features\n",
    "    filtered_data = pd.concat([metadata, features], axis=1)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "missing_values_filtered = missing_values_filter(\n",
    "    intensity_filtered, FEATURE_START_IDX, drop_cols_threshold=DROP_COLS_THRESHOLD\n",
    ")\n",
    "missing_values_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_COLS = [\"plate\", \"well\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_alignment_data(cell_data, batch_cols, feature_start_idx):\n",
    "    \"\"\"Prepare batch values and split metadata and feature DataFrames.\n",
    "\n",
    "    Args:\n",
    "        cell_data (pd.DataFrame): Input DataFrame containing metadata and features.\n",
    "        batch_cols (list): List of column names used to generate batch values.\n",
    "        feature_start_idx (int): Index where feature columns start.\n",
    "\n",
    "    Returns:\n",
    "        tuple: metadata (pd.DataFrame), features (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    # Create batch values\n",
    "    batch_values = cell_data[batch_cols[0]].astype(str)\n",
    "    for col in batch_cols[1:]:\n",
    "        batch_values = batch_values + \"_\" + cell_data[col].astype(str)\n",
    "\n",
    "    # Add batch values to metadata\n",
    "    metadata = cell_data.iloc[:, :feature_start_idx].copy()\n",
    "    metadata[\"batch_values\"] = batch_values\n",
    "\n",
    "    # Extract feature data\n",
    "    features = cell_data.iloc[:, feature_start_idx:].copy()\n",
    "\n",
    "    return features, metadata\n",
    "\n",
    "features, metadata = prepare_alignment_data(missing_values_filtered, BATCH_COLS, FEATURE_START_IDX)\n",
    "\n",
    "display(metadata)\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Recurion's EFAAR\n",
    "# code: https://github.com/recursionpharma/EFAAR_benchmarking/blob/trunk/efaar_benchmarking/efaar.py\n",
    "# paper: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012463\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import linalg\n",
    "\n",
    "def embed_by_pca(\n",
    "    features: np.ndarray,\n",
    "    metadata: pd.DataFrame = None,\n",
    "    variance_or_ncomp=128,\n",
    "    batch_col: str | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed the whole input data using principal component analysis (PCA).\n",
    "    Note that we explicitly center & scale the data (by batch) before an embedding operation with `PCA`.\n",
    "    Centering and scaling is done by batch if `batch_col` is not None, and on the whole data otherwise.\n",
    "    Also note that `PCA` transformer also does mean-centering on the whole data prior to the PCA operation.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Features to transform\n",
    "        metadata (pd.DataFrame): Metadata. Defaults to None.\n",
    "        variance_or_ncomp (float, optional): Variance or number of components to keep after PCA.\n",
    "            Defaults to 128 (n_components). If between 0 and 1, select the number of components such that\n",
    "            the amount of variance that needs to be explained is greater than the percentage specified.\n",
    "            If 1, a single component is kept, and if None, all components are kept.\n",
    "        batch_col (str, optional): Column name for batch information. Defaults to None.\n",
    "    Returns:\n",
    "        np.ndarray: Transformed data using PCA.\n",
    "    \"\"\"\n",
    "    features = features.copy()\n",
    "    features = centerscale_by_batch(features, metadata, batch_col)\n",
    "    features = PCA(variance_or_ncomp).fit_transform(features)\n",
    "    return features\n",
    "\n",
    "def tvn_on_controls(\n",
    "    embeddings: np.ndarray,\n",
    "    metadata: pd.DataFrame,\n",
    "    pert_col: str,\n",
    "    control_key: str,\n",
    "    batch_col: str | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply TVN (Typical Variation Normalization) to the data based on the control perturbation units.\n",
    "    Note that the data is first centered and scaled based on the control units.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): The embeddings to be normalized.\n",
    "        metadata (pd.DataFrame): The metadata containing information about the samples.\n",
    "        pert_col (str): The column name in the metadata DataFrame that represents the perturbation labels.\n",
    "        control_key (str): The control perturbation label.\n",
    "        batch_col (str, optional): Column name in the metadata DataFrame representing the batch labels\n",
    "            to be used for CORAL normalization. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The normalized embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.copy()\n",
    "    embeddings = centerscale_on_controls(embeddings, metadata, pert_col, control_key)\n",
    "    ctrl_ind = metadata[pert_col] == control_key\n",
    "    embeddings = PCA().fit(embeddings[ctrl_ind]).transform(embeddings)\n",
    "    embeddings = centerscale_on_controls(embeddings, metadata, pert_col, control_key, batch_col)\n",
    "    target_cov = np.cov(embeddings[ctrl_ind], rowvar=False, ddof=1) + 0.5 * np.eye(embeddings.shape[1])\n",
    "    if batch_col is not None:\n",
    "        batches = metadata[batch_col].unique()\n",
    "        for batch in batches:\n",
    "            batch_ind = metadata[batch_col] == batch\n",
    "            batch_control_ind = batch_ind & (metadata[pert_col] == control_key)\n",
    "            source_cov = np.cov(embeddings[batch_control_ind], rowvar=False, ddof=1) + 0.5 * np.eye(embeddings.shape[1])\n",
    "            embeddings[batch_ind] = np.matmul(embeddings[batch_ind], linalg.fractional_matrix_power(source_cov, -0.5))\n",
    "            embeddings[batch_ind] = np.matmul(embeddings[batch_ind], linalg.fractional_matrix_power(target_cov, 0.5))\n",
    "    return embeddings\n",
    "\n",
    "def centerscale_by_batch(\n",
    "    features: np.ndarray, metadata: pd.DataFrame = None, batch_col: str | None = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center and scale the input features by each batch. Not using any controls at all.\n",
    "    We are using this prior to embedding high-dimensional data with PCA.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Input features to be centered and scaled.\n",
    "        metadata (pd.DataFrame): Metadata information for the input features.\n",
    "        batch_col (str): Name of the column in metadata that contains batch information.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Centered and scaled features.\n",
    "    \"\"\"\n",
    "    features = features.copy()\n",
    "    if batch_col is None:\n",
    "        features = StandardScaler().fit_transform(features)\n",
    "    else:\n",
    "        if metadata is None:\n",
    "            raise ValueError(\"metadata must be provided if batch_col is not None\")\n",
    "        batches = metadata[batch_col].unique()\n",
    "        for batch in batches:\n",
    "            ind = metadata[batch_col] == batch\n",
    "            features[ind, :] = StandardScaler().fit_transform(features[ind, :])\n",
    "    return features\n",
    "\n",
    "def centerscale_on_controls(\n",
    "    embeddings: np.ndarray,\n",
    "    metadata: pd.DataFrame,\n",
    "    pert_col: str,\n",
    "    control_key: str,\n",
    "    batch_col: str | None = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center and scale the embeddings on the control perturbation units in the metadata.\n",
    "    If batch information is provided, the embeddings are centered and scaled by batch.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): The embeddings to be aligned.\n",
    "        metadata (pandas.DataFrame): The metadata containing information about the embeddings.\n",
    "        pert_col (str, optional): The column in the metadata containing perturbation information.\n",
    "        control_key (str, optional): The key for non-targeting controls in the metadata.\n",
    "        batch_col (str, optional): Column name in the metadata representing the batch labels.\n",
    "            Defaults to None.\n",
    "    Returns:\n",
    "        numpy.ndarray: The aligned embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = embeddings.copy()\n",
    "    if batch_col is not None:\n",
    "        batches = metadata[batch_col].unique()\n",
    "        for batch in batches:\n",
    "            batch_ind = metadata[batch_col] == batch\n",
    "            batch_control_ind = batch_ind & (metadata[pert_col] == control_key)\n",
    "            embeddings[batch_ind] = StandardScaler().fit(embeddings[batch_control_ind]).transform(embeddings[batch_ind])\n",
    "        return embeddings\n",
    "\n",
    "    control_ind = metadata[pert_col] == control_key\n",
    "    return StandardScaler().fit(embeddings[control_ind]).transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_variance_plot(features, variance_threshold=0.95, random_state=42):\n",
    "    \"\"\"Perform PCA analysis and create an explained variance plot.\n",
    "\n",
    "    Args:\n",
    "        feature_data (pd.DataFrame): DataFrame containing features.\n",
    "        variance_threshold (float): Cumulative variance threshold. Defaults to 0.95.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pca_df (pd.DataFrame): DataFrame with PCA-transformed data (gene symbols as index).\n",
    "            - n_components (int): Number of components needed to reach the variance threshold.\n",
    "            - pca_object (PCA): Fitted PCA object.\n",
    "            - fig (matplotlib.figure.Figure): Figure object for the explained variance plot.\n",
    "    \"\"\"\n",
    "    # Copy and scale data\n",
    "    features = features.copy()\n",
    "    features = centerscale_by_batch(features)\n",
    "\n",
    "    # Initialize and fit PCA\n",
    "    pca = PCA(random_state=random_state)\n",
    "    pca_transformed = pca.fit_transform(features)\n",
    "\n",
    "    # Create DataFrame with PCA results\n",
    "    n_components_total = pca_transformed.shape[1]\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca_transformed,\n",
    "        columns=[f\"pca_{n}\" for n in range(n_components_total)],\n",
    "    )\n",
    "\n",
    "    # Find number of components needed for threshold\n",
    "    cumsum = pca.explained_variance_ratio_.cumsum()\n",
    "    n_components = np.argwhere(cumsum >= variance_threshold)[0][0] + 1\n",
    "\n",
    "    # Create variance plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(cumsum, \"-\")\n",
    "    ax.axhline(\n",
    "        variance_threshold,\n",
    "        linestyle=\"--\",\n",
    "        color=\"red\",\n",
    "        label=f\"{variance_threshold * 100}% Threshold\",\n",
    "    )\n",
    "    ax.axvline(n_components, linestyle=\"--\", color=\"blue\", label=f\"n={n_components}\")\n",
    "    ax.set_ylabel(\"Cumulative fraction of variance explained\")\n",
    "    ax.set_xlabel(\"Number of principal components included\")\n",
    "    ax.set_title(\"PCA Explained Variance Ratio\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    print(\n",
    "        f\"Number of components needed for {variance_threshold * 100}% variance: {n_components}\"\n",
    "    )\n",
    "    print(f\"Shape of input data: {features.shape}\")\n",
    "\n",
    "    # Create threshold-limited version\n",
    "    pca_df_threshold = pca_df[[f\"pca_{i}\" for i in range(n_components)]]\n",
    "\n",
    "    print(f\"Shape of PCA transformed and reduced data: {pca_df_threshold.shape}\")\n",
    "\n",
    "    return pca_df_threshold, n_components, pca, fig\n",
    "\n",
    "pca_df_threshold, n_components, pca, fig = pca_variance_plot(features, variance_threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_COUNT = 432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[PERTURBATION_NAME_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_KEY = \"nontargeting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = embed_by_pca(features.values, metadata, variance_or_ncomp=PC_COUNT, batch_col=\"batch_values\")\n",
    "tvn_normalized = tvn_on_controls(pca_embeddings, metadata, PERTURBATION_NAME_COL, CONTROL_KEY, \"batch_values\")\n",
    "\n",
    "tvn_normalized_columns = [f'PCA_{i}' for i in range(tvn_normalized.shape[1])]\n",
    "tvn_normalized_df = pd.DataFrame(tvn_normalized, index=metadata.index, columns=tvn_normalized_columns)\n",
    "aligned_cell_data = pd.concat([metadata, tvn_normalized_df], axis=1)\n",
    "\n",
    "aligned_cell_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
